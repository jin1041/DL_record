{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.7942, -0.2543,  0.1098, -0.2321, -1.0910, -0.2508, -0.8620,\n",
       "            1.6607],\n",
       "          [ 1.4593,  0.6327,  0.1173, -0.5725, -0.4923, -1.3177,  0.9695,\n",
       "            0.2554],\n",
       "          [ 0.0973, -0.1601,  1.3130,  0.7643, -0.5781, -0.1826, -0.7810,\n",
       "            1.6035],\n",
       "          [ 0.0973, -0.1601,  1.3130,  0.7643, -0.5781, -0.1826, -0.7810,\n",
       "            1.6035]],\n",
       " \n",
       "         [[ 0.9817, -0.2296, -0.2355, -0.9257,  0.9638,  1.6550, -0.6574,\n",
       "           -0.3325],\n",
       "          [ 0.7942, -0.2543,  0.1098, -0.2321, -1.0910, -0.2508, -0.8620,\n",
       "            1.6607],\n",
       "          [ 0.6115,  1.4670,  0.4795,  0.5715, -0.4108, -0.1104, -0.5819,\n",
       "           -0.7797],\n",
       "          [ 1.4593,  0.6327,  0.1173, -0.5725, -0.4923, -1.3177,  0.9695,\n",
       "            0.2554]]], grad_fn=<EmbeddingBackward0>),\n",
       " tensor([[[ 0.6115,  1.4670,  0.4795,  0.5715, -0.4108, -0.1104, -0.5819,\n",
       "           -0.7797],\n",
       "          [ 0.7942, -0.2543,  0.1098, -0.2321, -1.0910, -0.2508, -0.8620,\n",
       "            1.6607],\n",
       "          [ 1.7706, -1.3445, -1.4392,  0.4921, -0.0899, -0.9106,  1.1281,\n",
       "            0.1214],\n",
       "          [-0.3558,  1.8649,  0.1882, -0.6467, -0.3905,  0.7402,  1.9894,\n",
       "            0.7712]],\n",
       " \n",
       "         [[ 1.7706, -1.3445, -1.4392,  0.4921, -0.0899, -0.9106,  1.1281,\n",
       "            0.1214],\n",
       "          [ 0.6115,  1.4670,  0.4795,  0.5715, -0.4108, -0.1104, -0.5819,\n",
       "           -0.7797],\n",
       "          [ 1.7706, -1.3445, -1.4392,  0.4921, -0.0899, -0.9106,  1.1281,\n",
       "            0.1214],\n",
       "          [ 0.0973, -0.1601,  1.3130,  0.7643, -0.5781, -0.1826, -0.7810,\n",
       "            1.6035]]], grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "num_src_words = 8\n",
    "num_tgt_words = 8\n",
    "model_dim = 8\n",
    "\n",
    "# Step1: word embedding\n",
    "src_len = torch.tensor([2, 4])\n",
    "tgt_len = torch.tensor([4, 3])\n",
    "\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(2, num_src_words, (L, )), (0, max(src_len) - L)), 0 )for L in src_len])\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(2, num_tgt_words, (L, )), (0, max(tgt_len) - L)), 0 )for L in tgt_len])\n",
    "\n",
    "src_embedding_table = nn.Embedding(num_src_words + 1, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(num_tgt_words + 1, model_dim)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = src_embedding_table(tgt_seq)\n",
    "\n",
    "src_embedding, tgt_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
       "\n",
       "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step2:position embedding\n",
    "max_position_len = 5\n",
    "pos_mat = torch.arange(max_position_len).reshape((-1, 1))\n",
    "i_mat = torch.arange(0, model_dim, 2).reshape((1, -1)) / model_dim\n",
    "i_mat = torch.pow(10000, i_mat)\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)),0) for L in src_len])\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "src_pe_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0878, 0.9122, 0.0000, 0.0000],\n",
       "         [0.1845, 0.8155, 0.0000, 0.0000],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
       "\n",
       "        [[0.0332, 0.1706, 0.5612, 0.2349],\n",
       "         [0.3350, 0.2386, 0.3562, 0.0702],\n",
       "         [0.4325, 0.3008, 0.1594, 0.1074],\n",
       "         [0.3641, 0.0343, 0.1105, 0.4911]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step3:mask encoder attention\n",
    "# valid_encoder_pos / valid_encoder_pos_matrix / \n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len) - L)), 0) for L in src_len]), 2)\n",
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_encoder_pos_matrix = 1 - valid_encoder_pos_matrix\n",
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "\n",
    "score = torch.randn( batch_size,max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step4:构造intra-attention的mask\n",
    "# Q @ K^T  [batch_size, tgt_len, src_len]\n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len) - L)), 0) for L in src_len]), 2)\n",
    "valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(tgt_len) - L)), 0) for L in tgt_len]), 2)\n",
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "invalid_cross_pos_matrix = 1 - valid_cross_pos_matrix\n",
    "mask_cross_attention = invalid_cross_pos_matrix.to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.9043, 0.0957, 0.0000, 0.0000],\n",
       "         [0.3807, 0.1550, 0.4643, 0.0000],\n",
       "         [0.1101, 0.4788, 0.1303, 0.2808]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3866, 0.6134, 0.0000, 0.0000],\n",
       "         [0.3449, 0.5049, 0.1502, 0.0000],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造decoder self-attention的mask\n",
    "# Q @ K^T\n",
    "valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones(L, L)), (0, max(tgt_len) - L, 0, max(tgt_len) - L)), 0) for L in tgt_len])\n",
    "invalid_decoder_tri_matrix = 1 - valid_decoder_tri_matrix\n",
    "invalid_decoder_tri_matrix = invalid_decoder_tri_matrix.to(torch.bool)\n",
    "score = torch.randn(batch_size, max(tgt_len), max(tgt_len))\n",
    "masked_score = score.masked_fill(invalid_decoder_tri_matrix, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(Q, K, V, atten_mask):\n",
    "    score = torch.bmm(Q, K.transpose(-2, -1)) / torch.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill(atten_mask, -1e9)\n",
    "    prob = F.softmax(masked_score, -1)\n",
    "    context = torch.bmm(prob, V)\n",
    "    return context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
